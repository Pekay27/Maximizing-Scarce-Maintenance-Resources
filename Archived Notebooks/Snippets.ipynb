{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# def impute(X):\n",
    "#     imputer = SimpleImputer(strategy='most_frequent')\n",
    "#     imputer.fit(processed_train)\n",
    "#     X['public_meeting'] = imputer.transform(X['public_meeting'])\n",
    "#     X['permit'] = X['permit'].fillna(False)\n",
    "#     X['scheme_management']\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_test = pd.DataFrame( impute(processed_test), columns=processed_test.columns)\n",
    "# processed_train = pd.DataFrame( impute(processed_train), columns=processed_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "# def standardize(X):\n",
    "#     # Features to standardize\n",
    "#     standardize_cols = ['amount_tsh', 'gps_height', 'longitude', \n",
    "#                         'latitude', 'population', 'pump_age', 'pop*amount_tsh', 'pop/amount_tsh', \n",
    "#                         'gps_height**2', 'gps_height**3', 'latitude*height*amount', 'latitude*height',\n",
    "#                        'gps_height_binned', 'amount_tsh_binned', 'longitude_binned', 'latitude_binned']\n",
    "    \n",
    "#     # Silence Data Conversion warning\n",
    "#     X[standardize_cols] = X[standardize_cols].astype(float)\n",
    "    \n",
    "#     # Fit and transform scaler\n",
    "#     scaler = RobustScaler()\n",
    "#     scaler.fit(train[standardize_cols])\n",
    "#     scaled = pd.DataFrame( scaler.transform(X[standardize_cols]) )\n",
    "    \n",
    "#     # Add back column names\n",
    "#     for i in range(len(standardize_cols)):\n",
    "#         scaled = scaled.rename(columns={i:standardize_cols[i]})\n",
    "        \n",
    "#     # Drop non-standardized columns\n",
    "#     X = X.drop(columns=standardize_cols)\n",
    "    \n",
    "#     # Concat scaled features with rest of features\n",
    "#     X = pd.concat([X, scaled], axis=1)\n",
    "    \n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = standardize(train)\n",
    "# test = standardize(test)\n",
    "\n",
    "# train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def target_encode(X):\n",
    "#     temp = X.copy()\n",
    "#     # Features to target encode\n",
    "#     target_cols = ['date_recorded_month', 'date_recorded_year', \n",
    "#                     'basin', 'region', 'extraction_type_class', \n",
    "#                     'management_group', 'payment', 'quality_group', \n",
    "#                     'quantity', 'source_type', 'source_class', \n",
    "#                     'waterpoint_type', 'scheme_management']\n",
    "    \n",
    "#     # Convert all relevant cols to category datatype (for encoder)\n",
    "#     X[target_cols] = X[target_cols].astype('category')\n",
    "    \n",
    "#     # Initialize and transform relevant features\n",
    "#     encoder = ce.TargetEncoder(smoothing=5, min_samples_leaf=5)\n",
    "    \n",
    "#     # Note, train hardcoded to avoid overfitting test data\n",
    "#     encoder.fit(train[target_cols], target['encoded'])\n",
    "#     encoded = encoder.transform(X[target_cols])\n",
    "    \n",
    "#     # Add Target Encoded features back to features DataFrame\n",
    "#     #X = X.drop(columns=target_cols)\n",
    "#     X = pd.concat([temp, encoded], axis=1)\n",
    "    \n",
    "#     return X## Polynomial Features\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# poly = PolynomialFeatures(interaction_only=False)\n",
    "# poly.fit(processed_train)\n",
    "# poly_train = poly.transform(processed_train)\n",
    "# poly_test = poly.transform(processed_test)\n",
    "# poly_train.shape, poly_test.shape\n",
    "\n",
    "# poly_train = pd.DataFrame(poly_train)\n",
    "# corr = pd.concat([poly_train, target['encoded']], axis=1)\n",
    "# corrs = corr.corr()\n",
    "# corrs['encoded'].nlargest(20)\n",
    "\n",
    "# corrs > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Polynomial Features\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# poly = PolynomialFeatures(interaction_only=False)\n",
    "# poly.fit(processed_train)\n",
    "# poly_train = poly.transform(processed_train)\n",
    "# poly_test = poly.transform(processed_test)\n",
    "# poly_train.shape, poly_test.shape\n",
    "\n",
    "# poly_train = pd.DataFrame(poly_train)\n",
    "# corr = pd.concat([poly_train, target['encoded']], axis=1)\n",
    "# corrs = corr.corr()\n",
    "# corrs['encoded'].nlargest(20)\n",
    "\n",
    "# corrs > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=15)\n",
    "\n",
    "# pca.fit(poly_train)\n",
    "\n",
    "# pca_train = pca.transform(poly_train)\n",
    "# pca_test = pca.transform(poly_test)\n",
    "\n",
    "# def pca_to_df(pca_array):\n",
    "#     pca_df = pd.DataFrame(pca_array)\n",
    "#     for col in pca_df.columns:\n",
    "#         pca_df = pca_df.rename(columns={\n",
    "#             col:'PCA_Poly' + str(col) \n",
    "#         })\n",
    "#     return pca_df\n",
    "\n",
    "# pca_train = pca_to_df(pca_train)\n",
    "# pca_test = pca_to_df(pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# K = 15\n",
    "# sum_of_squared_distances = []\n",
    "\n",
    "# km = KMeans(n_clusters=K)\n",
    "# km = km.fit(processed_train)\n",
    "# sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "# train_clusters = km.predict(processed_train)\n",
    "# test_clusters = km.predict(processed_test)\n",
    "\n",
    "# test_clusters = pd.Series(test_clusters, name='clusters')\n",
    "# train_clusters = pd.Series(train_clusters, name='clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenating DataFrame\n",
    "\n",
    "# mega_train = pd.concat([processed_train, pca_train, train_clusters], axis=1)\n",
    "# mega_test = pd.concat([processed_test, pca_test, test_clusters], axis=1)\n",
    "\n",
    "# mega_train.shape, mega_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "lr_scores = cross_validate(lr, \n",
    "                        processed_train, \n",
    "                        target['status_group'], \n",
    "                        return_train_score=True,\n",
    "                        return_estimator=True,\n",
    "                        scoring='accuracy', \n",
    "                        n_jobs=-1,\n",
    "                        cv=3)\n",
    "\n",
    "pd.DataFrame(lr_scores)\n",
    "\n",
    "lr_scores['estimator'][1]\n",
    "\n",
    "lr = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "lr.fit(processed_train, target['status_group'])\n",
    "\n",
    "classification_metrics(lr, processed_train, target['status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# scores = cross_validate(grboost, \n",
    "#                         processed_train, \n",
    "#                         target['status_group'],\n",
    "#                         verbose=10,\n",
    "#                         return_train_score=True,\n",
    "#                         return_estimator=True,\n",
    "#                         scoring='accuracy', \n",
    "#                         n_jobs=-1,\n",
    "#                         cv=3)\n",
    "\n",
    "# pd.DataFrame(scores)\n",
    "\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# param_distributions = {\n",
    "#     'n_estimators': [200, 300, 400],\n",
    "#     'max_depth': [15, 25, 30],\n",
    "# }\n",
    "\n",
    "# search = RandomizedSearchCV(model, \n",
    "#                            param_distributions=param_distributions,\n",
    "#                            n_iter=8,\n",
    "#                            scoring='accuracy', \n",
    "#                            verbose=10,\n",
    "#                            cv=3,\n",
    "#                            n_jobs=-1,\n",
    "#                            return_train_score=True)\n",
    "\n",
    "# search.fit(final_train, target['status_group'])\n",
    "\n",
    "# search.best_estimator_\n",
    "\n",
    "# search.best_score_\n",
    "\n",
    "### GradientBoostingClassifier \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(verbose=10, n_iter_no_change=10)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [15, 25],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(gbc, \n",
    "                           param_distributions=param_distributions,\n",
    "                           n_iter=8,\n",
    "                           scoring='accuracy', \n",
    "                           verbose=10,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "search.fit(final_train, target['status_group'])\n",
    "\n",
    "search.best_estimator_\n",
    "\n",
    "### SVC\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "param_distributions = {\n",
    "    'C': [0.1, 1],\n",
    "    'gamma': [0.01, 'auto'],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "svc_search = RandomizedSearchCV(svc, \n",
    "                           param_distributions=param_distributions,\n",
    "                           n_iter=8,\n",
    "                           scoring='accuracy', \n",
    "                           verbose=10,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "svc_search.fit(final_train, target['status_group'])\n",
    "\n",
    "svc.best_estimator_\n",
    "\n",
    "### K Nearest Neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "kn_search = RandomizedSearchCV(kn, \n",
    "                           param_distributions=param_distributions,\n",
    "                           n_iter=8,\n",
    "                           scoring='accuracy', \n",
    "                           verbose=10,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "kn_search.fit(final_train, target['status_group'])\n",
    "\n",
    "## Best Estimators\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
    "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=300, max_depth=15, n_iter_no_change=10)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, p=1, n_jobs=-1)\n",
    "\n",
    "svc = SVC(gamma='auto', C=1.0)\n",
    "\n",
    "## Voting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_cardinality_check(n, df):\n",
    "  \"\"\"\n",
    "  Given a cardinality limit (n) and a dataframe this function will search the\n",
    "  dataframe for features above the cardinality limit, then create a dict\n",
    "  from the results\n",
    "  \"\"\"\n",
    "  \n",
    "  feature_list = []\n",
    "  \n",
    "  cardinality_value = []\n",
    "  \n",
    "  for _ in range(len(df.columns)):\n",
    "    if len(df[df.columns[_]].value_counts()) > n:\n",
    "      \n",
    "      feature_list.append(df.columns[_])\n",
    "      \n",
    "      cardinality_value.append(len(df[df.columns[_]].value_counts()))\n",
    "                               \n",
    "        \n",
    "  feature_dict = dict(zip(feature_list, cardinality_value))\n",
    "  \n",
    "  return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  4  3  2  5\n",
       "6  3  6  5  4  7\n",
       "7  3  6  5  4  7\n",
       "8  3  6  5  4  7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(columns=[1,2,3,4,5], index=[6,7,8], data=[[3,4,5,6,7]])\n",
    "\n",
    "\n",
    "b = pd.DataFrame(columns=[1,4,3,2,5], index=[6,7,8], data=[[3,6,5,4,7]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = a.align(b, join='left', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5\n",
       "6  3  4  5  6  7\n",
       "7  3  4  5  6  7\n",
       "8  3  4  5  6  7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5\n",
       "6  3  4  5  6  7\n",
       "7  3  4  5  6  7\n",
       "8  3  4  5  6  7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num['distance'] = np.sqrt((train_num['longitude'] - mean_long_train)**2 + (train_num['latitude'] - mean_lat_train)**2) ",
    "test_num['distance'] = np.sqrt((test_num['longitude'] - mean_long_test)**2 + (test_num['latitude'] - mean_lat_test)**2) ",
    " ",
    "train_num['distance_height'] = np.sqrt((train_num['gps_height']**2 + train_num['longitude'] - mean_long_train)**2 + (train_num['latitude'] - mean_lat_train)**2) ",
    "test_num['distance_height'] = np.sqrt((test_num['gps_height']**2 + test_num['longitude'] - mean_long_test)**2 + (test_num['latitude'] - mean_lat_test)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
