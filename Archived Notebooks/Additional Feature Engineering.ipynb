{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 40), (14358, 40))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Data/train_features.csv')\n",
    "test = pd.read_csv('Data/test_features.csv')\n",
    "target = pd.read_csv('Data/train_labels.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['amount_tsh',\n",
    "    'date_recorded',\n",
    "    'gps_height',\n",
    "    'basin',\n",
    "    'region',\n",
    "    'population',\n",
    "    'public_meeting',\n",
    "    'scheme_management',\n",
    "    'permit',\n",
    "    'construction_year',\n",
    "    'extraction_type_class',\n",
    "    'management_group',\n",
    "    'payment',\n",
    "    'quality_group',\n",
    "    'quantity',\n",
    "    'source_type',\n",
    "    'source_class', \n",
    "    'waterpoint_type',\n",
    "    'funder',\n",
    "    'installer', \n",
    "    'latitude',\n",
    "    'longitude']\n",
    "\n",
    "def select_features(df, features):\n",
    "    '''\n",
    "    Subsets dataframe based on list of columns names accepted \n",
    "    as a parameter.\n",
    "    '''\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 22), (14358, 22))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = select_features(train, features=selected_features)\n",
    "test = select_features(test, features=selected_features)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "target['encoded'] = target['status_group'].replace({\n",
    "    'functional':1,\n",
    "    'non functional':-1,\n",
    "    'functional needs repair':0\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_features(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Create month and year features from the recorded data feature\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'])\n",
    "    X['date_recorded_month'] = X['date_recorded'].dt.month\n",
    "    X['date_recorded_year'] = X['date_recorded'].dt.year\n",
    "    \n",
    "    # Bin low freq. categories into 'other'\n",
    "    X['scheme_management'] = X['scheme_management'].replace({\n",
    "        'SWC':'Other',\n",
    "        'Trust':'Other',\n",
    "        'None':'Other'\n",
    "    })\n",
    "    \n",
    "    # Create age category out of construction_year\n",
    "    # Bin 0 values as -1\n",
    "    X['pump_age'] = ( 2014 - X['construction_year'] )\n",
    "    X['pump_age'].replace({2014:-1}).value_counts()\n",
    "    \n",
    "    # Alter longitude values so all points are reasonable\n",
    "    X['longitude'] = X['longitude'].replace({\n",
    "        0.000000:train['longitude'].median()\n",
    "    })\n",
    "    \n",
    "    # Create Installer Features\n",
    "    X['DWE_Installer'] = (X['installer'] == 'DWE')\n",
    "\n",
    "    one_time_install = train['installer'].value_counts()[train['installer'].value_counts() == 1]\n",
    "    X['One_Time_Installer'] = X['installer'].isin(one_time_install.index)\n",
    "\n",
    "    small_install = train['installer'].value_counts()[ (train['installer'].value_counts() < 10) & (train['installer'].value_counts() > 1) ]\n",
    "    X['Small_Installer'] = X['installer'].isin(small_install.index)\n",
    "\n",
    "    big_install = (( train['installer'].value_counts() >= 10 ) == True)\n",
    "    X['Big_Installer'] = X['installer'].isin(big_install.index)\n",
    "    \n",
    "    # Create Funder Features\n",
    "    X['Tanzania_Gov_Funder'] = (X['funder'] == 'Government Of Tanzania')\n",
    "\n",
    "    one_time_funder = train['funder'].value_counts()[train['funder'].value_counts() == 1]\n",
    "    X['One_Time_Funder'] = X['funder'].isin(one_time_funder.index)\n",
    "\n",
    "    small_funder = train['funder'].value_counts()[ (train['funder'].value_counts() < 10) & (train['funder'].value_counts() > 1) ]\n",
    "    X['Small_Funder'] = X['funder'].isin(small_funder.index)\n",
    "\n",
    "    big_funder = (( train['funder'].value_counts() >= 10 ) == True)\n",
    "    X['Big_Funder'] = X['funder'].isin(big_funder.index)\n",
    "    \n",
    "    # Replace population 0 with median population of Train\n",
    "    median_pop = train['population'].median()\n",
    "    X['population'] = X['population'].replace(0, median_pop)\n",
    "    \n",
    "    # Create interaction between amount of water avaialable and population\n",
    "    X['pop*amount_tsh'] = X['population'] * X['amount_tsh']\n",
    "    \n",
    "    # Drop unecessary columns\n",
    "    drop_cols = ['date_recorded', 'funder', 'installer', 'construction_year']\n",
    "    X = X.drop(columns=drop_cols)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 30), (14358, 30))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = wrangle_features(test)\n",
    "train = wrangle_features(train)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "def standardize(X):\n",
    "    # Features to standardize\n",
    "    standardize_cols = ['amount_tsh', 'gps_height', 'longitude', \n",
    "                        'latitude', 'population', ]\n",
    "    \n",
    "    # Silence Data Conversion warning\n",
    "    X[standardize_cols] = X[standardize_cols].astype(float)\n",
    "    \n",
    "    # Fit and transform scaler\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(train[standardize_cols])\n",
    "    scaled = pd.DataFrame( scaler.transform(X[standardize_cols]) )\n",
    "    \n",
    "    # Add back column names\n",
    "    for i in range(len(standardize_cols)):\n",
    "        scaled = scaled.rename(columns={i:standardize_cols[i]})\n",
    "        \n",
    "    # Drop non-standardized columns\n",
    "    X = X.drop(columns=standardize_cols)\n",
    "    \n",
    "    # Concat scaled features with rest of features\n",
    "    X = pd.concat([X, scaled], axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 30), (14358, 30))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = standardize(train)\n",
    "test = standardize(test)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "def one_hot(X):\n",
    "    # Features to one hot encode\n",
    "    one_hot_cols = ['date_recorded_month', 'date_recorded_year', \n",
    "                    'basin', 'region', 'extraction_type_class', \n",
    "                    'management_group', 'payment', 'quality_group', \n",
    "                    'quantity', 'source_type', 'source_class', \n",
    "                    'waterpoint_type']\n",
    "    \n",
    "    # Convert all relevant cols to category datatype (for encoder)\n",
    "    X[one_hot_cols] = X[one_hot_cols].astype('category')\n",
    "    \n",
    "    # Initialize and transform relevant features\n",
    "    encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "    \n",
    "    # Note, train hardcoded to avoid overfitting test data\n",
    "    encoder.fit(train[one_hot_cols])\n",
    "    X = encoder.transform(X[one_hot_cols])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(X):\n",
    "    # Features to target encode\n",
    "    target_cols = ['date_recorded_month', 'date_recorded_year', \n",
    "                    'basin', 'region', 'extraction_type_class', \n",
    "                    'management_group', 'payment', 'quality_group', \n",
    "                    'quantity', 'source_type', 'source_class', \n",
    "                    'waterpoint_type', 'scheme_management']\n",
    "    \n",
    "    # Convert all relevant cols to category datatype (for encoder)\n",
    "    X[target_cols] = X[target_cols].astype('category')\n",
    "    \n",
    "    # Initialize and transform relevant features\n",
    "    encoder = ce.TargetEncoder(smoothing=5, min_samples_leaf=5)\n",
    "    \n",
    "    # Note, train hardcoded to avoid overfitting test data\n",
    "    encoder.fit(train[target_cols], target['encoded'])\n",
    "    encoded = encoder.transform(X[target_cols])\n",
    "    \n",
    "    # Add Target Encoded features back to features DataFrame\n",
    "    X = X.drop(columns=target_cols)\n",
    "    X = pd.concat([X, encoded], axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 30), (14358, 30))"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test = target_encode(test)\n",
    "processed_train = target_encode(train)\n",
    "processed_train.shape, processed_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute(X):\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    imputer.fit(processed_train)\n",
    "    X = imputer.transform(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test = pd.DataFrame( impute(processed_test), columns=processed_test.columns)\n",
    "processed_train = pd.DataFrame( impute(processed_train), columns=processed_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train.to_csv(f'processed_train.csv', index=False)\n",
    "# processed_test.to_csv(f'processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 496), (14358, 496))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(interaction_only=False)\n",
    "poly.fit(processed_train)\n",
    "poly_train = poly.transform(processed_train)\n",
    "poly_test = poly.transform(processed_test)\n",
    "poly_train.shape, poly_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=15)\n",
    "\n",
    "pca.fit(poly_train)\n",
    "\n",
    "pca_train = pca.transform(poly_train)\n",
    "pca_test = pca.transform(poly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_to_df(pca_array):\n",
    "    pca_df = pd.DataFrame(pca_array)\n",
    "    for col in pca_df.columns:\n",
    "        pca_df = pca_df.rename(columns={\n",
    "            col:'PCA_Poly' + str(col) \n",
    "        })\n",
    "    return pca_df\n",
    "\n",
    "pca_train = pca_to_df(pca_train)\n",
    "pca_test = pca_to_df(pca_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K = [100]\n",
    "sum_of_squared_distances = []\n",
    "\n",
    "km = KMeans(n_clusters=k)\n",
    "km = km.fit(processed_train)\n",
    "sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "train_clusters = km.predict(processed_train)\n",
    "test_clusters = km.predict(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clusters = pd.Series(test_clusters, name='clusters')\n",
    "train_clusters = pd.Series(train_clusters, name='clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = pd.concat([processed_train, pca_train, train_clusters], axis=1)\n",
    "final_test = pd.concat([processed_test, pca_test, test_clusters], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 46), (14358, 46))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train.shape, final_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=300,\n",
    "                              max_depth=None, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  5.2min remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=300, n_jobs=-1,\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=None,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='warn', n_iter=8, n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [15, 25, 30],\n",
       "                                        'n_estimators': [200, 300, 400]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=True, scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [15, 25, 30],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(model, \n",
    "                           param_distributions=param_distributions,\n",
    "                           n_iter=8,\n",
    "                           scoring='accuracy', \n",
    "                           verbose=10,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "search.fit(final_train, target['status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.793989898989899"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(verbose=10, n_iter_no_change=10)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [15, 25],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(gbc, \n",
    "                           param_distributions=param_distributions,\n",
    "                           n_iter=8,\n",
    "                           scoring='accuracy', \n",
    "                           verbose=10,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "search.fit(final_train, target['status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "param_distributions = {\n",
    "    'C': [0.1, 1],\n",
    "    'gamma': [0.01, 'auto'],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "svc_search = RandomizedSearchCV(svc, \n",
    "                           param_distributions=param_distributions,\n",
    "                           n_iter=8,\n",
    "                           scoring='accuracy', \n",
    "                           verbose=10,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "svc_search.fit(final_train, target['status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "kn_search = RandomizedSearchCV(kn, \n",
    "                           param_distributions=param_distributions,\n",
    "                           n_iter=8,\n",
    "                           scoring='accuracy', \n",
    "                           verbose=10,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "kn_search.fit(final_train, target['status_group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
    "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=300, max_depth=15, n_iter_no_change=10)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, p=1, n_jobs=-1)\n",
    "\n",
    "svc = SVC(gamma='auto', C=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = [\n",
    "    ('knn', km),\n",
    "    ('svc', svc),\n",
    "    ('gbc', gbc),\n",
    "    ('rf', rf)\n",
    "]\n",
    "\n",
    "ensemble = VotingClassifier(estimators, n_jobs=-1, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(final_train, target['status_group'])\n",
    "ensemble_train_predict = ensemble.predict(final_train)\n",
    "ensemble_test_predict = ensemble.predict(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96493265993266"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target['status_group'], ensemble_train_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_test_pred):\n",
    "    sample_submission = pd.read_csv('Data/sample_submission.csv')\n",
    "    submission = sample_submission.copy()\n",
    "    submission['status_group'] = y_test_pred\n",
    "    \n",
    "    now = pd.to_datetime('now')\n",
    "    filename = 'MB_' + str(now).replace(' ','_')[0:-7] \n",
    "    \n",
    "    submission.to_csv(f'Submissions/{filename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(ensemble_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
