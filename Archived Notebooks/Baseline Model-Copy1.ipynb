{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 40), (14358, 40))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Data/train_features.csv')\n",
    "test = pd.read_csv('Data/test_features.csv')\n",
    "target = pd.read_csv('Data/train_labels.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['amount_tsh',\n",
    "    'date_recorded',\n",
    "    'gps_height',\n",
    "    'basin',\n",
    "    'region',\n",
    "    'population',\n",
    "    'public_meeting',\n",
    "    'scheme_management',\n",
    "    'permit',\n",
    "    'construction_year',\n",
    "    'extraction_type_class',\n",
    "    'management_group',\n",
    "    'payment',\n",
    "    'quality_group',\n",
    "    'quantity',\n",
    "    'source_type',\n",
    "    'source_class', \n",
    "    'waterpoint_type',\n",
    "    'funder',\n",
    "    'installer']\n",
    "\n",
    "def select_features(df, features):\n",
    "    '''\n",
    "    Subsets dataframe based on list of columns names accepted \n",
    "    as a parameter.\n",
    "    '''\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 20), (14358, 20))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = select_features(train, features=selected_features)\n",
    "test = select_features(test, features=selected_features)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_features(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Create month and year features from the recorded data feature\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'])\n",
    "    X['date_recorded_month'] = X['date_recorded'].dt.month\n",
    "    X['date_recorded_year'] = X['date_recorded'].dt.year\n",
    "    \n",
    "    # Bin low freq. categories into 'other'\n",
    "    X['scheme_management'] = X['scheme_management'].replace({\n",
    "        'SWC':'Other',\n",
    "        'Trust':'Other',\n",
    "        'None':'Other'\n",
    "    })\n",
    "    \n",
    "    # Create age category out of construction_year\n",
    "    # Bin 0 values as -1\n",
    "    X['pump_age'] = ( 2014 - X['construction_year'] )\n",
    "    X['pump_age'].replace({2014:-1}).value_counts()\n",
    "    \n",
    "    # Drop unecessary columns\n",
    "    drop_cols = ['date_recorded', 'construction_year']\n",
    "    X = X.drop(columns=drop_cols)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 19), (14358, 19))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = wrangle_features(train)\n",
    "test = wrangle_features(test)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def standardize(X):\n",
    "    # Features to standardize\n",
    "    standardize_cols = ['amount_tsh', 'gps_height', 'population', ]\n",
    "    \n",
    "    # Silence Data Conversion warning\n",
    "    X[standardize_cols] = X[standardize_cols].astype(float)\n",
    "    \n",
    "    # Fit and transform scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train[standardize_cols])\n",
    "    scaled = pd.DataFrame( scaler.transform(X[standardize_cols]) )\n",
    "    \n",
    "    # Add back column names\n",
    "    for i in range(len(standardize_cols)):\n",
    "        scaled = scaled.rename(columns={i:standardize_cols[i]})\n",
    "        \n",
    "    # Drop non-standardized columns\n",
    "    X = X.drop(columns=standardize_cols)\n",
    "    \n",
    "    # Concat scaled features with rest of features\n",
    "    X = pd.concat([X, scaled], axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 19), (14358, 19))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = standardize(train)\n",
    "test = standardize(test)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "def one_hot(X):\n",
    "    # Features to one hot encode\n",
    "    one_hot_cols = ['date_recorded_month', 'date_recorded_year', \n",
    "                    'basin', 'region', 'extraction_type_class', \n",
    "                    'management_group', 'payment', 'quality_group', \n",
    "                    'quantity', 'source_type', 'source_class', \n",
    "                    'waterpoint_type']\n",
    "    \n",
    "    # Convert all relevant cols to category datatype (for encoder)\n",
    "    X[one_hot_cols] = X[one_hot_cols].astype('category')\n",
    "    \n",
    "    # Initialize and transform relevant features\n",
    "    encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "    \n",
    "    # Note, train hardcoded to avoid overfitting test data\n",
    "    encoder.fit(train[one_hot_cols])\n",
    "    X = encoder.transform(X[one_hot_cols])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 94), (14358, 94))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train = one_hot(train)\n",
    "processed_test = one_hot(test)\n",
    "processed_train.shape, processed_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('Data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69572</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8776</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34310</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67743</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19728</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    status_group\n",
       "0  69572      functional\n",
       "1   8776      functional\n",
       "2  34310      functional\n",
       "3  67743  non functional\n",
       "4  19728      functional"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=300,\n",
    "                              max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(model, \n",
    "                        processed_train, \n",
    "                        target['status_group'], \n",
    "                        return_train_score=True,\n",
    "                        return_estimator=True,\n",
    "                        scoring='accuracy', \n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>estimator</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.965907</td>\n",
       "      <td>1.139960</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.778722</td>\n",
       "      <td>0.821692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.611494</td>\n",
       "      <td>1.130344</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.772157</td>\n",
       "      <td>0.823165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.540524</td>\n",
       "      <td>1.136692</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.774747</td>\n",
       "      <td>0.822727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.296544</td>\n",
       "      <td>1.173315</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.770286</td>\n",
       "      <td>0.823506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.210354</td>\n",
       "      <td>1.140754</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.770837</td>\n",
       "      <td>0.823366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time                                          estimator  \\\n",
       "0  23.965907    1.139960  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "1  23.611494    1.130344  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "2  24.540524    1.136692  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "3  31.296544    1.173315  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "4  25.210354    1.140754  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "\n",
       "   test_score  train_score  \n",
       "0    0.778722     0.821692  \n",
       "1    0.772157     0.823165  \n",
       "2    0.774747     0.822727  \n",
       "3    0.770286     0.823506  \n",
       "4    0.770837     0.823366  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score: 0.8197474747474748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.fit(processed_train, target['status_group'])\n",
    "\n",
    "y_train_pred = model.predict(processed_train)\n",
    "y_test_pred = model.predict(processed_test)\n",
    "\n",
    "score = accuracy_score(target['status_group'], y_train_pred)\n",
    "print(f'Train Accuracy Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grboost = GradientBoostingClassifier(n_estimators=100,\n",
    "                                    max_depth=3, \n",
    "                                    verbose=10,\n",
    "                                    n_iter_no_change=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       50991.8162            1.61m\n",
      "         2       48892.6589            1.58m\n",
      "         3       47155.6381            1.56m\n",
      "         4       45721.8270            1.54m\n",
      "         5       44517.4814            1.52m\n",
      "         6       43495.8115            1.50m\n",
      "         7       42635.8287            1.49m\n",
      "         8       41896.8445            1.47m\n",
      "         9       41234.9685            1.47m\n",
      "        10       40654.4576            1.47m\n",
      "        11       40159.6736            1.47m\n",
      "        12       39724.7802            1.46m\n",
      "        13       39330.3681            1.45m\n",
      "        14       38998.5898            1.44m\n",
      "        15       38668.6919            1.42m\n",
      "        16       38382.4507            1.41m\n",
      "        17       38120.3841            1.40m\n",
      "        18       37887.8508            1.38m\n",
      "        19       37668.4279            1.36m\n",
      "        20       37461.1813            1.35m\n",
      "        21       37258.4692            1.33m\n",
      "        22       37061.3256            1.32m\n",
      "        23       36915.3442            1.31m\n",
      "        24       36757.8223            1.29m\n",
      "        25       36634.5272            1.28m\n",
      "        26       36491.6897            1.27m\n",
      "        27       36366.3812            1.25m\n",
      "        28       36254.9036            1.24m\n",
      "        29       36146.3239            1.22m\n",
      "        30       36028.1173            1.20m\n",
      "        31       35942.0972            1.18m\n",
      "        32       35859.3339            1.17m\n",
      "        33       35769.3713            1.15m\n",
      "        34       35702.5116            1.14m\n",
      "        35       35625.1677            1.12m\n",
      "        36       35566.3830            1.10m\n",
      "        37       35485.3540            1.09m\n",
      "        38       35416.6236            1.07m\n",
      "        39       35346.3282            1.06m\n",
      "        40       35288.2250            1.05m\n",
      "        41       35235.9410            1.03m\n",
      "        42       35172.6940            1.01m\n",
      "        43       35121.1830           59.66s\n",
      "        44       35058.6337           58.53s\n",
      "        45       35001.0723           57.41s\n",
      "        46       34952.1628           56.22s\n",
      "        47       34898.4497           55.18s\n",
      "        48       34856.0836           54.05s\n",
      "        49       34810.0734           52.92s\n",
      "        50       34760.2037           51.79s\n",
      "        51       34723.7388           50.66s\n",
      "        52       34687.6468           49.52s\n",
      "        53       34643.1356           48.53s\n",
      "        54       34600.1987           47.46s\n",
      "        55       34567.0938           46.46s\n",
      "        56       34533.8105           45.36s\n",
      "        57       34502.4628           44.46s\n",
      "        58       34465.8400           43.44s\n",
      "        59       34428.2931           42.36s\n",
      "        60       34393.1145           41.31s\n",
      "        61       34348.8557           40.30s\n",
      "        62       34316.0556           39.24s\n",
      "        63       34286.1335           38.15s\n",
      "        64       34252.2687           37.08s\n",
      "        65       34207.8835           36.01s\n",
      "        66       34185.1475           34.93s\n",
      "        67       34141.4315           33.87s\n",
      "        68       34110.1585           32.85s\n",
      "        69       34065.1463           31.85s\n",
      "        70       34031.6202           30.81s\n",
      "        71       34007.8121           29.76s\n",
      "        72       33973.0870           28.74s\n",
      "        73       33949.7641           27.68s\n",
      "        74       33920.1810           26.64s\n",
      "        75       33877.3587           25.62s\n",
      "        76       33836.2765           24.60s\n",
      "        77       33810.1620           23.57s\n",
      "        78       33779.0329           22.52s\n",
      "        79       33740.6559           21.49s\n",
      "        80       33722.6949           20.44s\n",
      "        81       33697.9824           19.41s\n",
      "        82       33675.0098           18.37s\n",
      "        83       33656.8219           17.34s\n",
      "        84       33638.6269           16.33s\n",
      "        85       33619.3093           15.33s\n",
      "        86       33594.8828           14.30s\n",
      "        87       33574.6235           13.28s\n",
      "        88       33562.5221           12.26s\n",
      "        89       33542.1430           11.24s\n",
      "        90       33517.3437           10.20s\n",
      "        91       33498.4348            9.18s\n",
      "        92       33454.7369            8.15s\n",
      "        93       33432.7275            7.13s\n",
      "        94       33410.7650            6.10s\n",
      "        95       33387.2400            5.08s\n",
      "        96       33363.4071            4.06s\n",
      "        97       33345.4789            3.04s\n",
      "        98       33331.3388            2.03s\n",
      "        99       33313.0179            1.01s\n",
      "       100       33294.8824            0.00s\n",
      "Train Accuracy Score: 0.8197474747474748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "grboost.fit(processed_train, target['status_group'])\n",
    "\n",
    "y_train_pred = model.predict(processed_train)\n",
    "y_test_pred = model.predict(processed_test)\n",
    "\n",
    "score = accuracy_score(target['status_group'], y_train_pred)\n",
    "print(f'Train Accuracy Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y_test_pred):\n",
    "    sample_submission = pd.read_csv('Data/sample_submission.csv')\n",
    "    submission = sample_submission.copy()\n",
    "    submission['status_group'] = y_test_pred\n",
    "    \n",
    "    now = pd.to_datetime('now')\n",
    "    filename = 'MB_' + str(now).replace(' ','_')[0:-7] \n",
    "    \n",
    "    submission.to_csv(f'Submissions/{filename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    '''\n",
    "    Assumes at least 15 features, model must be fitted\n",
    "    '''\n",
    "    fi_values = model.feature_importances_\n",
    "    fi = pd.DataFrame({\n",
    "        'feature':processed_train.columns,\n",
    "        'importance':fi_values\n",
    "    })\n",
    "    fi['normalized'] = fi['importance'] / fi['importance'].sum()\n",
    "    fi = fi.sort_values('normalized', ascending=False).reset_index()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    ax = plt.subplot()\n",
    "    ax.barh(list(reversed(list(fi.index[:15]))),\n",
    "           fi['normalized'].head(15),\n",
    "           align='center', edgecolor='k')\n",
    "    ax.set_yticks( list(reversed(list(fi.index[:15]))) )\n",
    "    ax.set_yticklabels(fi['feature'].head(15))\n",
    "    ax.set_title('Top 15 Feature Importances')\n",
    "    \n",
    "    print(fi.tail(10))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
